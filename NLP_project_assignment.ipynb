{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5OsHIMNjzwSpGWitUOErR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NancyKibe211/AI4D-LAB-HACKATHON-CHALLENGE/blob/main/NLP_project_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing class project\n",
        "\n",
        "- Work in groups of 3. We will sign a list in class indicating the group members. \n",
        "\n",
        "- Submit your code in a text file and your results in a word/pdf document to liliomondi@gmail.com by 26th February 2023 \n",
        "\n",
        "- We will be using the Monday class on 20th  February to discuss questions regarding the project. \n",
        "\n",
        "__Question 1__  \n",
        "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  VP -> V NP | V NP PP\n",
        "  PP -> P NP\n",
        "  V -> \"saw\" | \"ate\" | \"walked\" | \n",
        "  V -> V NP | V NP PP | V Adv\n",
        "  PN -> \"John\" | \"Mary\" | \"Bob\" | \"Oscar\" | \"Paris\"\n",
        "  NP -> Det N | Det N PP\n",
        "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
        "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
        "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
        "  Adv -> \"suddenly\" | \"quickly\" | \"slowly |\"very\"\n",
        "  \"\"\")\n",
        "  \n",
        " \n",
        "sent = \"Mary saw Bob\".split()\n",
        "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
        "for tree in rd_parser.parse(sent):\n",
        "      print(tree)\n",
        "(S (NP Mary) (VP (V saw) (NP Bob)))\n",
        "\n",
        "\n",
        "*Improve on the grammar above so that it correctly gives the parse tree for the below sentences. \n",
        "Sally ate a sandwich .\n",
        "Sally and the president wanted and ate a sandwich .\n",
        "the very very very perplexed president ate a sandwich .\n",
        "Sally is lazy .\n",
        "Oscar died suddenly.\n",
        "The waiter put the chairs on the tables.\n",
        "Oscar called the waiter.\n",
        "Sally is eating a sandwich .\n",
        "You can edit your grammar in a text file, say mygrammar.cfg. You can then load it into NLTK and parse with it as follows:*  \n",
        "grammar1 = nltk.data.load('file:mygrammar.cfg')\n",
        "sent = \"Mary saw Bob\".split()\n",
        "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
        "for tree in rd_parser.parse(sent):\n",
        "      print(tree)\n",
        "\n",
        "\n",
        "- Show the parse tree for the above sentences."
      ],
      "metadata": {
        "id": "Svn36SZ_kmn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n",
        "import nltk\n",
        "nltk.download()"
      ],
      "metadata": {
        "id": "mEz2rcdfmIr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f2oKyWKsnAGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Natural Language Processing class project\n",
        "\n",
        "Work in groups of 3. We will sign a list in class indicating the group members. \n",
        "\n",
        "Submit your code in a text file and your results in a word/pdf document to liliomondi@gmail.com by 26th February 2023 \n",
        "\n",
        "We will be using the Monday class on 20th  February to discuss questions regarding the project. \n",
        "\n",
        "Question 1\"\"\"\n",
        "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  VP -> V NP | V NP PP\n",
        "  PP -> P NP\n",
        "  V -> \"saw\" | \"ate\" | \"walked\" | \n",
        "  V -> V NP | V NP PP | V Adv\n",
        "  PN -> \"John\" | \"Mary\" | \"Bob\" | \"Oscar\" | \"Paris\"\n",
        "  NP -> Det N | Det N PP\n",
        "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
        "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
        "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
        "  Adv -> \"suddenly\" | \"quickly\" | \"slowly |\"very\"\n",
        "  \"\"\")\n",
        " \n",
        "sent = \"Mary saw Bob\".split()\n",
        "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
        "for tree in rd_parser.parse(sent):\n",
        "      print(tree)\n",
        "(S (NP Mary) (VP (V saw) (NP Bob)))\n",
        "\"\"\"\n",
        "\n",
        "Improve on the grammar above so that it correctly gives the parse tree for the below sentences. \n",
        "Sally ate a sandwich .\n",
        "Sally and the president wanted and ate a sandwich .\n",
        "the very very very perplexed president ate a sandwich .\n",
        "Sally is lazy .\n",
        "Oscar died suddenly.\n",
        "The waiter put the chairs on the tables.\n",
        "Oscar called the waiter.\n",
        "Sally is eating a sandwich .\n",
        "You can edit your grammar in a text file, say mygrammar.cfg. You can then load it into NLTK and parse with it as follows:\n",
        "\"\"\"\n",
        "grammar1 = nltk.data.load('file:mygrammar.cfg')\n",
        "sent = \"Mary saw Bob\".split()\n",
        "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
        "for tree in rd_parser.parse(sent):\n",
        "      print(tree)\n",
        "\n",
        "\n",
        "#Show the parse tree for the above sentences."
      ],
      "metadata": {
        "id": "QD9IqizEk2TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## improved\n",
        "S -> NP VP  \n",
        "VP -> V | V NP | V NP PP | V Adv | VP   Conj VP   \n",
        "PP -> P NP  \n",
        "NP -> N | Det N | Det Adj N | PN | NP   PP | NP Conj NP\n",
        "Conj -> \"and\"  \n",
        "V -> \"ate\" | \"wanted\" | \"saw\" | \"walked\" | \"put\" | \"called\" | \"is\" | \"eating\" | \"died\"  \n",
        "PN -> \"Mary\" | \"Bob\" | \"Sally\" | \"president\" | \"Oscar\" | \"waiter\"  \n",
        "Det -> \"a\" | \"an\" | \"the\" | \"my\" | \"very\"  \n",
        "N -> \"sandwich\" | \"lazy\" | \"table\" | \"chair\" | \"telescope\" | \"park\" | \"president\"  \n",
        "P -> \"in\" | \"on\" | \"by\" | \"with\"  \n",
        "Adj -> \"perplexed\"  \n",
        "Adv -> \"suddenly\" | \"quickly\" | \"slowly\" | \"very\"  \n",
        "__Save in imporoved_grammer.txt__"
      ],
      "metadata": {
        "id": "Ysphn2uVncOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = nltk.data.load('file:improved_grammar.cfg')"
      ],
      "metadata": {
        "id": "L9GHaQOeoRJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" parse each sentence and print the resulting parse tree\"\"\"\n",
        "\n",
        "sentences = [    \"Sally ate a sandwich .\",    \"Sally and the president wanted and ate a sandwich .\",    \"the very very very perplexed president ate a sandwich .\",    \"Sally is lazy .\",    \"Oscar died suddenly .\",    \"The waiter put the chairs on the tables .\",    \"Oscar called the waiter .\",    \"Sally is eating a sandwich .\",]\n",
        "\n",
        "for sent in sentences:\n",
        "    tokens = sent.split()\n",
        "    parser = nltk.RecursiveDescentParser(grammar)\n",
        "    for tree in parser.parse(tokens):\n",
        "        tree.pretty_print()"
      ],
      "metadata": {
        "id": "GyudPBLyoghv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XOwP-EUmkUFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**\n",
        "- Choose several topics of your choice.\n",
        "- Download different webpages that talk about the chosen topics (5 to 10 pages) \n",
        "- Download a webpage online using the sample code below Save each webpage as a separate file. \n",
        "- import requests# install this using:  pip install requests \n",
        "- from bs4 import BeautifulSoup # install this using: pip install bs4\n",
        "\n",
        "- url = 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'  \n",
        "- #Make the request and check object type r = requests.get(url) type(r) \n",
        "- #Extract HTML from Response object and print html = r.text \n",
        "- #print(html) \n",
        "- #Create a BeautifulSoup object from the HTML soup = BeautifulSoup(html, \"html5lib\") type(soup) \n",
        "- #Get soup title soup.title  \n",
        "- #Get soup title as string soup.title.string  \n",
        "- #Get the text out of the soup and print it text = soup.get_text() #print(text)   For each text perform \n",
        "1. Tokenize \n",
        "1. Lowercase \n",
        "1. Remove stop-words \n",
        "1. Plot the frequency distribution of the words in the different documents \n",
        "1. Calculate the tf-idf for each document in your corpus \n",
        "1. For each document, show the top 10 words with the highest tf.idf values \n",
        "1. Use cosine similarity to find the most similar documents in your text. HINT create a matrix of similarities where each document is compared to every other sentence. Give the top 5 most similar documents in your corpus. You should see that documents that talk about the same topic are more similar to documents that talk about different topics."
      ],
      "metadata": {
        "id": "zVqdJvLopS2D"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RCrIWOiFtJo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "\n",
        "# Define the URLs to download\n",
        "urls = [\n",
        "    'https://en.m.wikipedia.org/wiki/Atmospheric_temperature',\n",
        "    'https://psl.noaa.gov/thredds/catalog/Datasets/cpc_us_precip/catalog.html',\n",
        "    'https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/',\n",
        "    'https://medium.com/analytics-vidhya/multi-seasonal-time-series-analysis-decomposition-and-forecasting-with-python-609409570007',\n",
        "    'https://psl.noaa.gov/data/gridded/data.ncep.reanalysis.html',\n",
        "    'https://psl.noaa.gov/enso/mei/',\n",
        "    'https://byjus.com/free-ias-prep/ncert-notes-factors-controlling-temperature-distribution/'\n",
        "]\n",
        "\n",
        "# Download each page and save as a file\n",
        "\"\"\"This will save each page as a separate HTML file in the current directory.\"\"\"\n",
        "\n",
        "for i, url in enumerate(urls):\n",
        "    r = requests.get(url)\n",
        "    with open(f'page{i}.html', 'w') as f:\n",
        "        f.write(r.text)"
      ],
      "metadata": {
        "id": "HoYLGHfgqz-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Using nltk to tokenize, lowercase, and remove stop words from the text:\n",
        "\"\"\"\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Load the stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load the text from a file\n",
        "with open('page0.html') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Lowercase the tokens\n",
        "tokens = [t.lower() for t in tokens]\n",
        "\n",
        "# Remove stop words\n",
        "tokens = [t for t in tokens if t not in stop_words]"
      ],
      "metadata": {
        "id": "QqpTkeyzrVHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"plot the frequency distribution of the words, we can use the nltk.FreqDist() method \"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the frequency distribution of the tokens\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "\n",
        "# Plot the frequency distribution\n",
        "fdist.plot(30, cumulative=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dn_KAGTLrmM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "To calculate the tf-idf for each document, we can use the TfidfVectorizer class from the sklearn.feature_extraction.text module:\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the text from all files\n",
        "texts = []\n",
        "for i in range(7):\n",
        "    with open(f'page{i}.html') as f:\n",
        "        texts.append(f.read())\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Calculate the tf-idf for all documents\n",
        "tfidf_matrix = tfidf.fit_transform(texts)\n",
        "\n",
        "# Get the feature names (i.e., the words)\n",
        "feature_names = tfidf.get_feature_names()\n",
        "\n",
        "# Get the tf-idf values for the first document\n",
        "doc_id = 0\n",
        "doc_tfidf = tfidf_matrix[doc_id]\n",
        "\n",
        "# Get the top 10 words with the highest tf-idf values\n",
        "sorted_items = sort_coo(doc_tfidf.tocoo())\n",
        "top_items = get_top_n(sorted_items, 10)\n",
        "\n",
        "# Print the top 10 words\n",
        "for idx, score in top_items:\n",
        "    print(f\"{feature_names[idx]}: {score}\")"
      ],
      "metadata": {
        "id": "NAYNs6jssECP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Here are the helper functions used in the code above\n",
        "\"\"\"\n",
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "\n",
        "def get_top_n(sorted_items, n):\n",
        "    return [(i, score) for (i, score) in sorted_items[:n]]\n",
        "\n",
        "\n",
        "def get_similarities(tfidf_matrix):\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    return cosine_similarity(tfidf_matrix)"
      ],
      "metadata": {
        "id": "5Cj-TFLtsWAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" To use cosine similarity to find the most similar documents in the corpus, we can use the cosine_similarity function from the sklearn.metrics.pairwise module: \n",
        "\"\"\"\n",
        "# Calculate the cosine similarity between all documents\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# Get the most similar documents for each document\n",
        "for i in range(len(texts)):\n",
        "    # Get the most similar documents for document i\n",
        "    sim_indices = similarity_matrix[i].argsort()[:-6:-1]\n",
        "    sim_indices = sim_indices[sim_indices != i]\n",
        "    sim_scores = similarity_matrix[i][sim_indices]\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Most similar documents to page{i}.html:\")\n",
        "    for j, score in zip(sim_indices, sim_scores):\n",
        "        print(f\" - page{j}.html (score: {score:.2f})\")\n",
        "\n",
        "\"\"\"\n",
        "This will print the five most similar documents for each document in the corpus. The score represents the cosine similarity between the two documents, with higher scores indicating greater similarity.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kIR8RYonsl6M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}